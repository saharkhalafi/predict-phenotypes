# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_flw_Ylcz8yh7e2wpDv0uf--jZ5cFWRa
"""

import os
import math
from sklearn.model_selection import train_test_split
#import numpy as np
import pandas as pd
import torch
from torch.nn import BCEWithLogitsLoss
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from tqdm import tqdm, trange
import matplotlib.pyplot as plt
# %matplotlib inline
from pickle import load
from numpy import array
import nltk
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.vis_utils import plot_model
import os
from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU
from keras.callbacks import Callback
from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten ,RepeatVector,TimeDistributed
from keras.preprocessing import text, sequence
from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D
from keras import initializers, regularizers, constraints, optimizers, layers, callbacks
from keras.callbacks import EarlyStopping,ModelCheckpoint
from keras.models import Model
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score  
from keras.optimizers import Adam, RMSprop
from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D
from tensorflow import keras
# %matplotlib inline
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.metrics import fbeta_score
from statistics import mean
from sklearn.metrics import hamming_loss
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import learning_curve
from sklearn.metrics import roc_auc_score, confusion_matrix
from nltk.stem.wordnet import WordNetLemmatizer
from timeit import default_timer as timer
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import re
import sys
import warnings


torch.cuda.empty_cache()

print("GPU Available: {}".format(torch.cuda.is_available()))
n_gpu = torch.cuda.device_count()
print("Number of GPU Available: {}".format(n_gpu))
print("GPU: {}".format(torch.cuda.get_device_name(0)))

df2 = pd.read_csv('drive/My Drive/phenomdataset.csv')
df2 = df2.loc[:, ~df2.columns.str.contains('^Unnamed')]



"""Data cleaning"""

nltk.download('stopwords')

df2['TEXT2'] = df2.TEXT.str.lower().str.replace('\W', ' ')

df2['TEXT2'] = df2.TEXT2.str.replace('\s+', ' ', regex=True)

from string import digits 
remove_digits = str.maketrans('', '', digits)
df2['TEXT2'] = df2.TEXT2.str.translate(remove_digits)

def removestop(line):
  line=str(line)
  clean_line = ""
  for char in line:
       if char in 'qwertyuiopasdfghjklzxcvbnm':
           clean_line += char
       else:
           clean_line += ' '
  clean_line = re.sub(' +',' ',clean_line)
  words = clean_line.lower().split()                             
  stops = set(stopwords.words("english"))                  
  
  #stops = set(col_one_list) 
  meaningful_words = [w for w in words if not w in stops]  
  return( " ".join( meaningful_words ))

df2['TEXT2'] = df2['TEXT2'].apply(removestop)

def Punctuation(string): 
  
    # punctuation marks 
    punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~0123456789'''
  
    # traverse the given string and if any punctuation 
    # marks occur replace it with null 
    for x in string.lower(): 
        if x in punctuations: 
            string = string.replace(x, "") 
  
    # Print string without punctuation 
    print(string)


w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
    meaningful_words2=[lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]
    return( " ".join( meaningful_words2 ))


df2['TEXT2'] = df2['TEXT2'].apply(lemmatize_text)

nltk.download('punkt')

from nltk.tokenize import word_tokenize
revie_lines=list()
df2['TEXT2']=df2['TEXT2'].astype(str)
lines=df2['TEXT2'].values.tolist()
for line in lines:
  line=str(line)
  tokens=word_tokenize(line)
  tokens=[w.lower() for w in tokens]
  revie_lines.append(tokens)




#pretraining word embeddings

from gensim.models import FastText
model_fast=FastText(revie_lines,size=300,window=5,workers=4,sg=1,iter=15)
words2=list(model_fast.wv.vocab)
len(words2)

model_fast.wv.most_similar('cancer')

model_fast.wv.save_word2vec_format('drive/My Drive/fasttext300d.txt',binary=False)

import gensim
model_wo=gensim.models.Word2Vec(sentences=revie_lines,size=300,window=5,workers=4,sg=1,iter=15)
words=list(model_wo.wv.vocab)
len(words)

model_wo.wv.most_similar('cancer')

model_wo.wv.save_word2vec_format('drive/My Drive/wordembeddingssskipgram.txt',binary=False)

fast_file = open('drive/My Drive/fasttext300d.txt', encoding="utf8")
embeddings_dictionary={}
for line in fast_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:])
    embeddings_dictionary[word] = vector_dimensions
fast_file.close()

word_file = open('drive/My Drive/wordembeddingssskipgram.txt', encoding="utf8")
embeddings_dictionary2={}
for line2 in word_file:
    records2 = line2.split()
    word2 = records2[0]
    vector_dimensions2 = np.asarray(records2[1:])
    embeddings_dictionary2[word2] = vector_dimensions2
word_file.close()


tokenize2=Tokenizer()
df2['TEXT2']=df2['TEXT2'].astype(str)
datas=df2['TEXT2']
tokenize2.fit_on_texts(datas)
max_length=max([len(s.split()) for s in datas])
sequences = tokenize2.texts_to_sequences(df2['TEXT2'])
wordindex=tokenize2.word_index

vocabulary_size=len(wordindex)+1
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(df2['TEXT2'])
sequences = tokenizer.texts_to_sequences(df2['TEXT2'])
wordindex=tokenizer.word_index
data = pad_sequences(sequences, maxlen=max_length)
labels = df2[['Obesity','Advanced.Heart.Disease',
       'Advanced.Lung.Disease','Schizophrenia.and.other.Psychiatric.Disorders','Alcohol.Abuse','Other.Substance.Abuse',
       'Chronic.Pain.Fibromyalgia','Chronic.Neurological.Dystrophies','Advanced.Cancer','Depression']].values

#labels = df2[['Depression']].values


embedding_matrix = np.zeros((vocabulary_size, 300))
for word, index in wordindex.items():
    if index > vocabulary_size - 1:
        break
    else:
        embedding_vector = embeddings_dictionary.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector

embedding_matrix2 = np.zeros((vocabulary_size, 300))
for word2, index2 in wordindex.items():
    if index2 > vocabulary_size - 1:
        break
    else:
        embedding_vector2 = embeddings_dictionary2.get(word2)
        if embedding_vector2 is not None:
            embedding_matrix2[index2] = embedding_vector2

embedding_matrix2.shape

X_train, X_test, Y_train, Y_test = train_test_split(data,labels, test_size = 0.2, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

def build_model():

    inp = Input(shape = (max_length,))

    xe = Embedding(vocabulary_size, 300, weights = [embedding_matrix2], trainable = False)(inp)
    #x3 = Embedding(vocabulary_size, 300, weights = [embedding_matrix], trainable = False)(inp

    x =Bidirectional(GRU(512, return_sequences=True))(xe)
    #x=TimeDistributed(Dense(128,activation='relu'))(x)
    #x=Flatten()(x)
    #x=Dense(128,activation='relu')(x)
    x = Conv1D(512,kernel_size = 2, padding = "valid", kernel_initializer = "he_uniform")(x)
    #x = MaxPooling1D()(x)
    #x = Dropout(0.5)(x)
    #x = LSTM(256,return_sequences=True)(x)
    #flatten_layer = Flatten()
    #x = flatten_layer(x)   # instantiate the layer
    #x = RepeatVector(1)(x)
    #x = Bidirectional(LSTM(128, return_sequences=True))(x)
    x2 = Conv1D(512, kernel_size = 2,activation='relu')(xe)

    max_pool1 = GlobalMaxPooling1D()(x)
    max_pool1 = Dropout(0.4)(max_pool1)
    
    #flatten_layer = Flatten() 
    #flat1 = flatten_layer(max_pool1)   
    #flat1 = Flatten(max_pool1)
    #xx = Dense(128, activation='relu')(max_pool1)
    
    max_pool2 = GlobalMaxPooling1D()(x2)
    max_pool2 = Dropout(0.6)(max_pool2)
    #flat2 = flatten_layer(max_pool2)
    #xxx = Dense(512, activation='relu')(max_pool2)
    
    xd = Dense(512, activation='relu')(max_pool2)
    
    xp = concatenate([max_pool1,xd])
    
    x = Dense(10, activation = "sigmoid")(xp)

    model = Model(inputs = inp, outputs = x)
    #print(model.summary())
    plot_model(model, to_file='drive/MyDrive/imprv22bilstmsnn.png', show_shapes=True, show_layer_names=True)
    #opt = adam(lr=0.01, decay=1e-6)

    model.compile(loss = "binary_crossentropy", optimizer ='adam' , metrics=[tfa.metrics.F1Score(num_classes=10,average="micro",threshold=0.1)])

    history = model.fit(X_train, Y_train, batch_size=64, epochs=100, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001)]) 

    return model

model=build_model()

yhat_probs2 = model.predict(X_test)
pred_probs3=np.round(yhat_probs2)
pred_probs3=pred_probs3.astype(int)
pred_probs3[1]

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve
f1macro=f1_score(Y_test, pred_probs3, average='macro')
f1micro=f1_score(Y_test, pred_probs3
                 , average='micro')
recalll=recall_score(Y_test, pred_probs3, average='macro')
pree=precision_score(Y_test, pred_probs3, average='macro')
print('micro',f1micro)
print('macro',f1macro)
print('recall',recalll)
print('pre',pree)